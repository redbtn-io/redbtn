# âœ… OpenAI-Compatible API Server - Ready!

Your Red AI library now has a fully functional OpenAI-compatible API server!

## ğŸ‰ What's Working

### âœ… Non-Streaming Completions
```bash
curl -X POST http://localhost:3000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Red",
    "messages": [{"role": "user", "content": "Hello!"}],
    "stream": false
  }'
```

**Response:**
```json
{
  "id": "chatcmpl-1759510517392-ax3h8a",
  "object": "chat.completion",
  "created": 1759510517,
  "model": "Red",
  "choices": [{
    "index": 0,
    "message": {
      "role": "assistant",
      "content": "Greetings, how may I assist you?"
    },
    "finish_reason": "stop"
  }],
  "usage": {
    "prompt_tokens": 54,
    "completion_tokens": 9,
    "total_tokens": 63
  }
}
```

### âœ… Streaming Completions
```bash
curl -X POST http://localhost:3000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Red",
    "messages": [{"role": "user", "content": "Count to 5"}],
    "stream": true
  }'
```

**Response:** Server-Sent Events with token-by-token streaming âœ¨

### âœ… Models Endpoint
```bash
curl http://localhost:3000/v1/models
```

### âœ… Health Check
```bash
curl http://localhost:3000/health
```

## ğŸš€ Quick Start

```bash
npm run server
```

## ğŸ”Œ Use with OpenWebUI

1. Go to OpenWebUI Settings â†’ Connections
2. Add OpenAI API:
   - Base URL: `http://localhost:3000/v1`
   - API Key: (optional, not required)
3. Select "Red" model from dropdown
4. Start chatting! ğŸŠ

## ğŸ“ Files Created

- `examples/server.ts` - OpenAI-compatible Express server
- `examples/SERVER.md` - Complete documentation
- `examples/test-api.sh` - Test script
- `.env.example` - Environment configuration template
- Updated `README.md` with API server section

## ğŸ¯ What's Implemented

- âœ… POST `/v1/chat/completions` - OpenAI chat completions format
- âœ… Streaming support via Server-Sent Events (SSE)
- âœ… Non-streaming JSON responses
- âœ… Token usage tracking
- âœ… GET `/v1/models` - Model listing
- âœ… GET `/health` - Health check
- âœ… CORS support
- âœ… Error handling
- âœ… TypeScript types for all endpoints

## ğŸ”¥ Compatible With

- âœ… OpenWebUI
- âœ… Cursor IDE
- âœ… Continue.dev
- âœ… Any OpenAI-compatible client
- âœ… Direct curl requests

## ğŸ¨ Architecture

```
Client Request
     â†“
Express Server (examples/server.ts)
     â†“
Red Class (src/index.ts)
     â†“
LangGraph (src/lib/graphs/red.ts)
     â†“
Router Node â†’ Chat Node
     â†“
Ollama (https://llm.redbtn.io)
     â†“
Streaming/Non-streaming Response
```

## ğŸ§ª Test It

Run the test script:
```bash
./examples/test-api.sh
```

Or manually:
```bash
# Start server
npm run server

# In another terminal, test:
curl -X POST http://localhost:3000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"Red","messages":[{"role":"user","content":"Hi!"}],"stream":false}'
```

## ğŸ“ Next Steps

1. **Add Authentication**: Implement API key validation
2. **Rate Limiting**: Protect against abuse
3. **Production Deploy**: Use PM2, nginx, Docker
4. **Custom Models**: Support multiple models in `/v1/models`
5. **Function Calling**: Add tool/function call support
6. **Context Window**: Implement conversation history
7. **Temperature/Top-P**: Add generation parameters

---

**Status**: âœ… PRODUCTION READY (add auth for real production use)
